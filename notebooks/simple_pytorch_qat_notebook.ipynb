{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple PyTorch QAT with Gemma-3-1B-IT\n",
    "\n",
    "Minimal quantization-aware training setup using `torch.ao.quantization` with the Gemma-3-270m-IT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.ao.quantization import get_default_qat_qconfig, prepare_qat, convert\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/gemma-3-270m-it...\n",
      "Model loaded with 268098176 parameters\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "model_name = \"google/gemma-3-270m-it\"\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.float32,\n",
    "    device_map=None,\n",
    ")\n",
    "\n",
    "# Add padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded with {sum(p.numel() for p in model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 5 examples\n"
     ]
    }
   ],
   "source": [
    "# Simple training data\n",
    "train_texts = [\n",
    "    \"What is machine learning? Machine learning is a subset of AI that enables computers to learn from data.\",\n",
    "    \"Explain quantum computing. Quantum computing uses quantum mechanics to process information.\",\n",
    "    \"What is Python? Python is a high-level programming language known for its simplicity.\",\n",
    "    \"Define neural networks. Neural networks are computing systems inspired by biological neural networks.\",\n",
    "    \"What is deep learning? Deep learning is a subset of machine learning using neural networks.\",\n",
    "]\n",
    "\n",
    "\n",
    "def prepare_batch(texts, tokenizer, max_length=128):\n",
    "    \"\"\"Tokenize and prepare batch for training.\"\"\"\n",
    "    encoded = tokenizer(\n",
    "        texts, max_length=max_length, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": encoded[\"input_ids\"].to(device),\n",
    "        \"attention_mask\": encoded[\"attention_mask\"].to(device),\n",
    "        \"labels\": encoded[\"input_ids\"].to(device),  # For causal LM\n",
    "    }\n",
    "\n",
    "\n",
    "print(f\"Training on {len(train_texts)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing model for quantization-aware training...\n",
      "QAT model prepared with 268098176 parameters\n"
     ]
    }
   ],
   "source": [
    "# Setup QAT configuration\n",
    "model.train()\n",
    "\n",
    "# Configure quantization\n",
    "qconfig = get_default_qat_qconfig(\"qnnpack\")  # Use fbgemm backend\n",
    "model.qconfig = qconfig\n",
    "\n",
    "# Prepare model for QAT\n",
    "print(\"Preparing model for quantization-aware training...\")\n",
    "qat_model = prepare_qat(model, inplace=False)\n",
    "qat_model = qat_model.to(device)\n",
    "\n",
    "print(\n",
    "    f\"QAT model prepared with {sum(p.numel() for p in qat_model.parameters())} parameters\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting QAT training for 2 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 5/5 [00:08<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average Loss = 2.8353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 5/5 [00:06<00:00,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Average Loss = 1.3583\n",
      "Training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "optimizer = optim.AdamW(qat_model.parameters(), lr=1e-5)\n",
    "num_epochs = 2\n",
    "batch_size = 1  # Small batch for demo\n",
    "\n",
    "print(f\"Starting QAT training for {num_epochs} epochs...\")\n",
    "\n",
    "training_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "\n",
    "    # Simple batching\n",
    "    for i in tqdm(range(0, len(train_texts), batch_size), desc=f\"Epoch {epoch+1}\"):\n",
    "        batch_texts = train_texts[i : i + batch_size]\n",
    "\n",
    "        # Prepare batch\n",
    "        batch = prepare_batch(batch_texts, tokenizer)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = qat_model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(qat_model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    training_losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}: Average Loss = {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to quantized model...\n",
      "Original model size: 1022.71 MB\n",
      "Quantized model size: 640.21 MB\n",
      "Compression ratio: 1.60x\n"
     ]
    }
   ],
   "source": [
    "# Convert to quantized model\n",
    "print(\"Converting to quantized model...\")\n",
    "torch.backends.quantized.engine = \"qnnpack\"\n",
    "quantized_model = convert(qat_model.eval(), inplace=False)\n",
    "\n",
    "\n",
    "# Compare model sizes\n",
    "def get_model_size(model):\n",
    "    \"\"\"Get model size in MB.\"\"\"\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    return param_size / (1024 * 1024)\n",
    "\n",
    "\n",
    "original_size = get_model_size(model)\n",
    "quantized_size = get_model_size(quantized_model)\n",
    "compression_ratio = original_size / quantized_size\n",
    "\n",
    "print(f\"Original model size: {original_size:.2f} MB\")\n",
    "print(f\"Quantized model size: {quantized_size:.2f} MB\")\n",
    "print(f\"Compression ratio: {compression_ratio:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Notebook completed successfully!\n",
      "Loaded Gemma-3-1B-IT model\n",
      "Applied quantization-aware training\n",
      "Achieved 1.60x compression\n",
      "Training loss reduced by 52.09%\n"
     ]
    }
   ],
   "source": [
    "# Save the quantized model (optional)\n",
    "# torch.save(quantized_model.state_dict(), 'gemma_3_1b_quantized.pth')\n",
    "# print(\"Quantized model saved to 'gemma_3_1b_quantized.pth'\")\n",
    "\n",
    "print(\"\\nNotebook completed successfully!\")\n",
    "print(f\"Loaded Gemma-3-1B-IT model\")\n",
    "print(f\"Applied quantization-aware training\")\n",
    "print(f\"Achieved {compression_ratio:.2f}x compression\")\n",
    "print(\n",
    "    f\"Training loss reduced by {((training_losses[0] - training_losses[-1]) / training_losses[0] * 100):.2f}%\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
